{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcij9sj4mExxy+0MR1pF35",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcmachicaocuf/codigos_CUF_LLM_NLP/blob/main/U4__LLM__herramientas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Fine-tuning vs. RAG (Búsqueda con recuperación de contexto)\n"
      ],
      "metadata": {
        "id": "bkiFTtbkfmy1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpIM31NHfbHL"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import wikipedia\n",
        "\n",
        "# RAG-style retrieval\n",
        "def retrieve_context(query):\n",
        "    results = wikipedia.search(query)\n",
        "    if results:\n",
        "        summary = wikipedia.summary(results[0], sentences=2)\n",
        "        return summary\n",
        "    return \"No context found.\"\n",
        "\n",
        "# Generación con contexto\n",
        "qa_pipeline = pipeline(\"text-generation\", model=\"facebook/opt-125m\")\n",
        "\n",
        "query = \"¿Quién fue Nikola Tesla?\"\n",
        "context = retrieve_context(query)\n",
        "response = qa_pipeline(f\"Contexto: {context} Pregunta: {query} Respuesta:\")\n",
        "\n",
        "print(response[0]['generated_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Cuantización para reducir tamaño de modelos"
      ],
      "metadata": {
        "id": "mqVr1Y9mftjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Carga del modelo en precisión reducida\n",
        "model_id = \"facebook/opt-125m\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "\n",
        "print(f\"Modelo {model_id} cargado con cuantización en float16\")"
      ],
      "metadata": {
        "id": "ok7SMG0cfuvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Simulación de RLHF (Entrenamiento con retroalimentación humana)"
      ],
      "metadata": {
        "id": "uYi5Exo2ftgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Simulación de feedback humano en RLHF\n",
        "responses = [\"Buena respuesta\", \"Respuesta irrelevante\", \"Respuesta útil\", \"Incorrecto\"]\n",
        "rewards = {\"Buena respuesta\": 1.0, \"Respuesta útil\": 0.5, \"Incorrecto\": -1.0, \"Respuesta irrelevante\": -0.5}\n",
        "\n",
        "# Simulación de entrenamiento\n",
        "def simulate_feedback():\n",
        "    response = np.random.choice(responses)\n",
        "    return response, rewards[response]\n",
        "\n",
        "for _ in range(5):\n",
        "    response, reward = simulate_feedback()\n",
        "    print(f\"Modelo generó: {response} | Recompensa: {reward}\")"
      ],
      "metadata": {
        "id": "sBOSa0yEfvzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. IA Simbólica con Reglas y LLM"
      ],
      "metadata": {
        "id": "iirCS8Puftdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def symbolic_reasoning(question):\n",
        "    rules = {\n",
        "        \"Es un mamífero y pone huevos\": \"ornitorrinco\",\n",
        "        \"Tiene plumas y vuela\": \"ave\",\n",
        "        \"Tiene escamas y vive en agua\": \"pez\"\n",
        "    }\n",
        "\n",
        "    for rule, answer in rules.items():\n",
        "        if rule in question:\n",
        "            return f\"Según el razonamiento simbólico, la respuesta es: {answer}\"\n",
        "\n",
        "    return \"No se encontró una respuesta basada en reglas.\"\n",
        "\n",
        "print(symbolic_reasoning(\"Es un mamífero y pone huevos\"))"
      ],
      "metadata": {
        "id": "DY7b7ETWgJvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Interacción entre agentes LLM"
      ],
      "metadata": {
        "id": "6AYu0tpbftap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "\n",
        "    def respond(self, message):\n",
        "        return f\"{self.name} responde: '{message[::-1]}'\"  # Simulación de respuesta\n",
        "\n",
        "agent1 = Agent(\"Agente A\")\n",
        "agent2 = Agent(\"Agente B\")\n",
        "\n",
        "message = \"Hola, cómo estás?\"\n",
        "print(agent1.respond(message))\n",
        "print(agent2.respond(agent1.respond(message)))"
      ],
      "metadata": {
        "id": "yZVBo_d4gSLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Uso de LangChain para generación de reportes estructurados"
      ],
      "metadata": {
        "id": "ntdMrrIIftQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI  # O usar un modelo local como GPT4All\n",
        "\n",
        "# Definir plantilla de informe estructurado\n",
        "template = \"\"\"\n",
        "Eres un asistente especializado en generar reportes técnicos.\n",
        "Responde con información clara y estructurada.\n",
        "\n",
        "Tema: {topic}\n",
        "\n",
        "[Resumen]\n",
        "Escribe un resumen del tema en 2-3 líneas.\n",
        "\n",
        "[Datos clave]\n",
        "Enumera 3 puntos clave sobre el tema.\n",
        "\n",
        "[Conclusión]\n",
        "Da una conclusión concisa y bien fundamentada.\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(input_variables=[\"topic\"], template=template)\n",
        "llm = OpenAI(model_name=\"gpt-3.5-turbo\")  # Cambiar por otro LLM si es necesario\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Solicitar informe sobre un tema\n",
        "response = chain.run(topic=\"Impacto de la IA en la educación\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "UlFemuadgbDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. LangGraph para decisión basada en reglas con LLM\n"
      ],
      "metadata": {
        "id": "X3_9i-FrgdvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Definir estados\n",
        "class State:\n",
        "    def __init__(self, query):\n",
        "        self.query = query\n",
        "        self.result = None\n",
        "\n",
        "# Nodo 1: Buscar en una base de datos antes de generar texto\n",
        "def search_database(state):\n",
        "    knowledge_base = {\n",
        "        \"IA en salud\": \"Los modelos de IA se usan en diagnóstico y personalización de tratamientos.\",\n",
        "        \"Automatización en fábricas\": \"Los robots industriales han aumentado la eficiencia en la manufactura.\"\n",
        "    }\n",
        "    if state.query in knowledge_base:\n",
        "        state.result = knowledge_base[state.query]\n",
        "        return END  # Termina aquí si hay respuesta en la base de datos\n",
        "    return \"llm_generation\"  # Pasa al siguiente paso\n",
        "\n",
        "# Nodo 2: Generar texto si no hay información previa\n",
        "def llm_generation(state):\n",
        "    llm = OpenAI(model_name=\"gpt-3.5-turbo\")\n",
        "    state.result = llm.predict(f\"Explica brevemente: {state.query}\")\n",
        "    return END\n",
        "\n",
        "# Construcción del grafo\n",
        "graph = StateGraph(State)\n",
        "graph.add_node(\"search_database\", search_database)\n",
        "graph.add_node(\"llm_generation\", llm_generation)\n",
        "graph.set_entry_point(\"search_database\")\n",
        "graph.add_edge(\"search_database\", \"llm_generation\")\n",
        "\n",
        "# Ejecutar flujo\n",
        "state = State(\"IA en salud\")\n",
        "graph.run(state)\n",
        "print(state.result)"
      ],
      "metadata": {
        "id": "-jIo0Ym_ghfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # 8. LangChain para estructurar contratos en JSON modulares"
      ],
      "metadata": {
        "id": "MExB8W-ygt1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(model_name=\"gpt-4\")  # Usar modelo adecuado\n",
        "\n",
        "# Nivel 1: Análisis de estructura contractual\n",
        "structure_prompt = PromptTemplate(\n",
        "    input_variables=[\"contract_text\"],\n",
        "    template=\"Extrae la estructura del contrato en un JSON con las cláusulas y sus relaciones clave.\"\n",
        ")\n",
        "structure_chain = LLMChain(llm=llm, prompt=structure_prompt)\n",
        "\n",
        "# Nivel 2: Análisis de riesgos\n",
        "risk_prompt = PromptTemplate(\n",
        "    input_variables=[\"contract_structure\"],\n",
        "    template=\"Analiza los riesgos en base a la estructura dada y devuelve un JSON con los principales riesgos.\"\n",
        ")\n",
        "risk_chain = LLMChain(llm=llm, prompt=risk_prompt)\n",
        "\n",
        "# Ejemplo de entrada (contrato ficticio)\n",
        "contract_text = \"El arrendador se obliga a entregar el inmueble en buen estado. El arrendatario debe pagar mensualmente.\"\n",
        "\n",
        "# Generación de JSON de estructura\n",
        "json_structure = structure_chain.run(contract_text)\n",
        "print(\"Estructura contractual:\", json_structure)\n",
        "\n",
        "# Generación de JSON de riesgos (basado en estructura previa)\n",
        "json_risks = risk_chain.run(json_structure)\n",
        "print(\"Análisis de riesgos:\", json_risks)"
      ],
      "metadata": {
        "id": "i4Bonn4ygteA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Código: Gestión Modular de Proyectos con JSON"
      ],
      "metadata": {
        "id": "E1YxlxhWg3S5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Definir células JSON independientes\n",
        "task_json = {\n",
        "    \"tasks\": [\n",
        "        {\"id\": 1, \"name\": \"Diseño UI\", \"dependency\": None, \"duration\": 10},\n",
        "        {\"id\": 2, \"name\": \"Backend API\", \"dependency\": 1, \"duration\": 15},\n",
        "        {\"id\": 3, \"name\": \"Pruebas\", \"dependency\": 2, \"duration\": 7}\n",
        "    ]\n",
        "}\n",
        "\n",
        "risk_json = {\n",
        "    \"risks\": [\n",
        "        {\"task_id\": 2, \"risk\": \"Retraso en integración con frontend\", \"probability\": 0.3},\n",
        "        {\"task_id\": 3, \"risk\": \"Errores críticos en pruebas\", \"probability\": 0.5}\n",
        "    ]\n",
        "}\n",
        "\n",
        "resource_json = {\n",
        "    \"resources\": [\n",
        "        {\"id\": 1, \"name\": \"Diseñador UX\", \"assigned_to\": 1},\n",
        "        {\"id\": 2, \"name\": \"Desarrollador Backend\", \"assigned_to\": 2},\n",
        "        {\"id\": 3, \"name\": \"QA Tester\", \"assigned_to\": 3}\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Función para articular información de diferentes células\n",
        "def generate_project_view(view_type):\n",
        "    if view_type == \"timeline\":\n",
        "        return json.dumps(task_json, indent=4)\n",
        "\n",
        "    elif view_type == \"risk_analysis\":\n",
        "        return json.dumps(risk_json, indent=4)\n",
        "\n",
        "    elif view_type == \"resource_allocation\":\n",
        "        return json.dumps(resource_json, indent=4)\n",
        "\n",
        "    elif view_type == \"full_project\":\n",
        "        project_data = {**task_json, **risk_json, **resource_json}\n",
        "        return json.dumps(project_data, indent=4)\n",
        "\n",
        "    else:\n",
        "        return \"Vista no reconocida.\"\n",
        "\n",
        "# Ejemplo de uso: Generar vista de riesgos\n",
        "print(\"🔍 Análisis de Riesgos:\")\n",
        "print(generate_project_view(\"risk_analysis\"))\n",
        "\n",
        "# Ejemplo de uso: Generar vista completa del proyecto\n",
        "print(\"\\n📊 Vista Completa del Proyecto:\")\n",
        "print(generate_project_view(\"full_project\"))"
      ],
      "metadata": {
        "id": "HfiFeQWkg4Cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Ejemplo de Código con Dos Ontologías de Calidad"
      ],
      "metadata": {
        "id": "AlVwYynmhH63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Definimos dos ontologías de calidad\n",
        "quality_ontologies = {\n",
        "    \"ISO9001\": {\n",
        "        \"focus\": \"Procesos Estandarizados\",\n",
        "        \"priorities\": [\"Documentación\", \"Estandarización\", \"Auditoría\"],\n",
        "        \"workflow_adjustments\": {\"add_review_phase\": True, \"reduce_iterations\": False}\n",
        "    },\n",
        "    \"AgileQuality\": {\n",
        "        \"focus\": \"Flexibilidad e Iteración\",\n",
        "        \"priorities\": [\"Colaboración\", \"Iteraciones Rápidas\", \"Feedback\"],\n",
        "        \"workflow_adjustments\": {\"add_review_phase\": False, \"reduce_iterations\": True}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Plan inicial sin ajustes de calidad\n",
        "project_plan = {\n",
        "    \"tasks\": [\n",
        "        {\"id\": 1, \"name\": \"Diseño\", \"duration\": 10},\n",
        "        {\"id\": 2, \"name\": \"Desarrollo\", \"duration\": 15},\n",
        "        {\"id\": 3, \"name\": \"Pruebas\", \"duration\": 7}\n",
        "    ]\n",
        "}\n",
        "\n",
        "def apply_quality_ontology(ontology_name):\n",
        "    \"\"\"Ajusta el plan según la ontología de calidad seleccionada.\"\"\"\n",
        "    ontology = quality_ontologies.get(ontology_name)\n",
        "    if not ontology:\n",
        "        return \"Ontología no reconocida.\"\n",
        "\n",
        "    modified_plan = project_plan.copy()\n",
        "\n",
        "    if ontology[\"workflow_adjustments\"][\"add_review_phase\"]:\n",
        "        modified_plan[\"tasks\"].append({\"id\": 4, \"name\": \"Revisión Final\", \"duration\": 5})\n",
        "\n",
        "    if ontology[\"workflow_adjustments\"][\"reduce_iterations\"]:\n",
        "        for task in modified_plan[\"tasks\"]:\n",
        "            task[\"duration\"] = max(1, int(task[\"duration\"] * 0.8))  # Reducimos duración en 20%\n",
        "\n",
        "    return json.dumps(modified_plan, indent=4)\n",
        "\n",
        "# Comparación entre dos estrategias\n",
        "print(\"🔍 Estrategia ISO9001:\")\n",
        "print(apply_quality_ontology(\"ISO9001\"))\n",
        "\n",
        "print(\"\\n🚀 Estrategia AgileQuality:\")\n",
        "print(apply_quality_ontology(\"AgileQuality\"))"
      ],
      "metadata": {
        "id": "HXiupVf7hIUc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}